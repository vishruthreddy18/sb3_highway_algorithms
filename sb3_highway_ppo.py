# -*- coding: utf-8 -*-
"""sb3_highway_ppo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11ycK4aDk-Vh4yQmJNGwRK9dT-bts50TD

# Highway with SB3's DQN
* Example code from Highway-env
##  Warming up
We start with a few useful installs and imports:
"""

# Commented out IPython magic to ensure Python compatibility.
# Install environment and agent
!pip install highway-env
# TODO: we use the bleeding edge version because the current stable version does not support the latest gym>=0.21 versions. Revert back to stable at the next SB3 release.
!pip install git+https://github.com/DLR-RM/stable-baselines3

# Environment
import gymnasium as gym
import highway_env

# Agent
from stable_baselines3 import PPO

# Visualization utils
# %load_ext tensorboard
import sys
from tqdm.notebook import trange
!pip install tensorboardx gym pyvirtualdisplay
!apt-get install -y xvfb python3-opengl ffmpeg
!git clone https://github.com/eleurent/highway-env.git 2> /dev/null
sys.path.insert(0, '/content/highway-env/scripts/')
from utils import record_videos, show_videos

"""## Training
Run tensorboard, cell 7, ***immediately after*** starting
cell 8, so there is training output to read

"""

# model = PPO("MlpPolicy", "highway-fast-v0",
#             policy_kwargs=dict(net_arch=[dict(pi=[256, 256], vf=[256, 256])]),
#             n_steps=128,
#             batch_size=64,
#             n_epochs=10,
#             learning_rate=5e-4,
#             gamma=0.8,
#             verbose=2,
#             tensorboard_log="highway_ppo/",)
# model.learn(int(2e4))

model = PPO("MlpPolicy", "highway-fast-v0",
            policy_kwargs=dict(net_arch=[dict(pi=[256, 256], vf=[256, 256])]),
            n_steps=256,
            batch_size=128,
            n_epochs=5,
            learning_rate=1e-4,
            gamma=0.9,
            verbose=2,
            tensorboard_log="highway_ppo/",)
model.learn(int(2e4))

"""## Testing

Visualize a few episodes
"""

env = gym.make("highway-fast-v0", render_mode='rgb_array')
env = record_videos(env)
for _ in range(5):
    obs, info = env.reset()
    done = truncated = False
    while not (done or truncated):
        action, _ = model.predict(obs)
        obs, reward, done, truncated, info = env.step(action)
env.close()
show_videos()

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir "highway_ppo"

"""**Assignment**
* Train a DQN agent to survive at least 20 seconds in the highway-fast-v0 env
  * Document your training process
* Evaluate the effects of a low and a high hyperparameter value for learning rate, batch_size and target_update_interval
  * Report and explain results
* Choose another RL algorithm (PPO, A2C, DDPG,...) available in stable-baselines3 and use it to train an agent in the highway-fast-v0 env
  * Report, explain and contrast your results with those of your original DQN agent
* Documentation should include plots from tensor board, videos, etc. Make sure to revise the log directory (i.e. *tensorboard_log*) every time you run experiments with different settings so the results will be stored separately. Use tensorboard to visualize the training results and compare model performance.  Project report should provide rationales for choices and experiments and explanations for results, particularly those that were unexpected.
* For extra credit you may try additional learning algorithms or experiment with improving agent performance in other ways (e,g, novel model structure, observation space, self-attention, LSTM)
"""